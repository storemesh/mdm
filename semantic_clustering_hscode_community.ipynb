{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import DBSCAN, OPTICS, KMeans\n",
    "from sklearn import metrics\n",
    "import hdbscan\n",
    "import time\n",
    "import umap\n",
    "import umap.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('dataset/semantic_similarity/hscode.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hscode2022 = df[df['HSYear'] == 2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hscode4_th = df_hscode2022[['thdescriptions4','HSCode2']]\n",
    "df_hscode4_th = df_hscode4_th.rename(columns={'thdescriptions4': 'hscode4_text'})\n",
    "\n",
    "df_hscode4_en = df_hscode2022[['endescriptions4','HSCode2']]\n",
    "df_hscode4_en = df_hscode4_en.rename(columns={'endescriptions4': 'hscode4_text'})\n",
    "\n",
    "df_hscode4 = pd.concat([\n",
    "    df_hscode4_th,\n",
    "    df_hscode4_en\n",
    "])\n",
    "df_hscode4 = df_hscode4.drop_duplicates()\n",
    "df_hscode4 = df_hscode4[df_hscode4['hscode4_text'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model.encode(df_hscode4['hscode4_text'].values, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2427, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_accuracy(series):    \n",
    "    n = 2\n",
    "    count_series = series.value_counts()\n",
    "    sum_all = count_series.sum()\n",
    "    count_series_no_noise = count_series[count_series.index != -1]\n",
    "    sum_top_n = count_series_no_noise.iloc[:n].sum()\n",
    "    accuracy = sum_top_n / sum_all\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_community(threshold):\n",
    "    clusters = util.community_detection(corpus_embeddings, min_community_size=5, threshold=threshold)\n",
    "    df_hscode4['cluster_group_id'] = -1\n",
    "    \n",
    "    for i, cluster in enumerate(clusters):\n",
    "        df_hscode4.iloc[cluster, df_hscode4.columns.get_loc('cluster_group_id')] = i\n",
    "    \n",
    "    acc_each_hscode = df_hscode4.groupby('HSCode2')['cluster_group_id'].apply(find_accuracy)\n",
    "    # import pdb;pdb.set_trace()\n",
    "    acc_mean = acc_each_hscode.mean()\n",
    "    \n",
    "    n_group = df_hscode4['cluster_group_id'].value_counts().shape[0]\n",
    "    if n_group == 1:\n",
    "        silhouette = 0.0\n",
    "    # import pdb; pdb.set_trace()\n",
    "    else:\n",
    "        silhouette = metrics.silhouette_score(X, df_hscode4['cluster_group_id'].values, metric='euclidean')\n",
    "    \n",
    "    return acc_each_hscode, acc_mean, silhouette, df_hscode4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.15, acc_mean: 0.8958, silhouette: -0.1145, cluster_size: 34\n",
      "threshold: 0.2, acc_mean: 0.8375, silhouette: -0.07093, cluster_size: 47\n",
      "threshold: 0.25, acc_mean: 0.733, silhouette: -0.08876, cluster_size: 64\n",
      "threshold: 0.3, acc_mean: 0.6011, silhouette: -0.0828, cluster_size: 88\n",
      "threshold: 0.35, acc_mean: 0.482, silhouette: -0.08549, cluster_size: 115\n",
      "threshold: 0.4, acc_mean: 0.4446, silhouette: -0.07822, cluster_size: 136\n",
      "threshold: 0.45, acc_mean: 0.4052, silhouette: -0.07011, cluster_size: 158\n",
      "threshold: 0.5, acc_mean: 0.3854, silhouette: -0.06904, cluster_size: 167\n",
      "threshold: 0.55, acc_mean: 0.3503, silhouette: -0.07348, cluster_size: 165\n",
      "threshold: 0.6, acc_mean: 0.328, silhouette: -0.08341, cluster_size: 148\n",
      "threshold: 0.65, acc_mean: 0.2414, silhouette: -0.124, cluster_size: 107\n",
      "threshold: 0.7, acc_mean: 0.1574, silhouette: -0.1593, cluster_size: 64\n",
      "threshold: 0.75, acc_mean: 0.1047, silhouette: -0.1789, cluster_size: 42\n",
      "threshold: 0.8, acc_mean: 0.04468, silhouette: -0.1812, cluster_size: 18\n",
      "threshold: 0.85, acc_mean: 0.02285, silhouette: -0.1816, cluster_size: 11\n",
      "threshold: 0.9, acc_mean: 0.004759, silhouette: -0.05689, cluster_size: 4\n",
      "threshold: 0.95, acc_mean: 0.0, silhouette: 0.0, cluster_size: 1\n"
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "for threshold in np.arange(0.15, 1.0, 0.05):    \n",
    "    acc_each_hscode, acc_mean, silhouette, df_result = find_community(threshold=threshold)\n",
    "    n_each_cluster = df_result['cluster_group_id'].value_counts()\n",
    "    n_top_20_cluster = n_each_cluster.iloc[:20].values\n",
    "    cluster_size = n_each_cluster.shape[0]\n",
    "    result_list.append({\n",
    "        'threshold': threshold,\n",
    "        'acc_mean': acc_mean,\n",
    "        'silhouette': silhouette,\n",
    "        'cluster_size': cluster_size,\n",
    "        'n_top_20_cluster': n_top_20_cluster,\n",
    "    })\n",
    "    print(f'threshold: {threshold:.04}, acc_mean: {acc_mean:.04}, silhouette: {silhouette:.04}, cluster_size: {cluster_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_all = pd.DataFrame(result_list)\n",
    "df_result_all.to_csv('commnity_detection_output.csv',index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_each_hscode, acc_mean, silhouette, df_result = find_community(threshold=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('mdm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "c2efc473d0a0d50c19a5adbeac08e391174523270bbcd5ecc02a4aa5f4c81327"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
